{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVM-level automatic differentiation\n",
    "This notebook shows how to use tvm-level automatic differentiation and discusses how it works internally, what you can expect to be differentiated well, and what still requires some more work. Note that this is a work-in-progress and the result of differentiating certain operations is not as performant yet as we want it to be.\n",
    "\n",
    "Let's start by importing modules and defining some helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "import topi\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def get_shape(tensor):\n",
    "    return [tvm.ir_pass.Simplify(s).value for s in tensor.shape]\n",
    "\n",
    "# This function builds a tvm function, runs it for several iterations, \n",
    "# and returns a string with the median time and some additional statistics\n",
    "def measure_performance(outputs, inputs, min_seconds=1):\n",
    "    sched = tvm.create_schedule([o.op for o in outputs])\n",
    "    mout = tvm.build(sched, outputs + inputs)\n",
    "    \n",
    "    arguments = [tvm.nd.empty(get_shape(t), t.dtype) for t in outputs + inputs]\n",
    "    \n",
    "    times = []\n",
    "    while sum(times) < min_seconds:\n",
    "        before = time.time()\n",
    "        mout(*arguments)\n",
    "        after = time.time()\n",
    "        times.append(after - before)\n",
    "        \n",
    "    return \"{}ms median, avg={}, std={} ({} iters)\".format(\n",
    "        int(1000*np.median(times)), int(1000*np.mean(times)), int(1000*np.std(times)), len(times))\n",
    "\n",
    "# Print the lowered representation\n",
    "def show_lowered(outputs, inputs):\n",
    "    sout = tvm.create_schedule([o.op for o in outputs])\n",
    "    mout = tvm.lower(sout, outputs + inputs, simple_mode=True)\n",
    "    print(mout)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use automatic differentiation\n",
    "Basically, all you need is the function `tvm.differentiate` which takes a tensor, differentiates it with respect to other given tensors using reverse accumulation, and applies certain optimizations. Let's consider an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "X = tvm.placeholder((32, 10000), name='X')\n",
    "W = tvm.placeholder((3000, 10000), name='W')\n",
    "B = tvm.placeholder((3000,), name='B')\n",
    "\n",
    "# output\n",
    "Y = topi.nn.dense(X, W, B)\n",
    "\n",
    "# Adjoint (head gradients). In this case it is has the same shape as Y and\n",
    "# represents the gradient of some hypothetical scalar loss with respect to Y.\n",
    "# In the most common case Y will be the loss itself with the shape (1,)\n",
    "# and H will simply be a scalar 1, but here we want to look at a more general case.\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "\n",
    "# Get Jacobians of Y wrt W and B, multiplied by H,\n",
    "# in other words, get gradients of some loss wrt W and B\n",
    "# given H, the gradient of this loss wrt Y\n",
    "[dW, dB] = tvm.differentiate(Y, [W, B], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  863ms median, avg=863, std=28 (2 iters)\n",
      "backward 566ms median, avg=566, std=31 (2 iters)\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X, B, W]))\n",
    "print(\"backward\", measure_performance([dW, dB], [X, B, W, H]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `H` may be omitted, and then the Jacobian itself will be returned (which is called gradient when Y is a scalar). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = topi.sum(Y)\n",
    "res = tvm.differentiate(loss, [W, B])\n",
    "[dW, dB] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result `res` mimics a list, but it also contains a couple of additional fields, namely `adjoints` and `adjoint_summands`. The `adjoints` dict maps original tensors to corresponding adjoints (gradients of the output with respect to this particular tensor):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Tensor(shape=[3000, 10000], op.name=W): Tensor(shape=[3000, 10000], op.name=compute.W.grad),\n",
       " Tensor(shape=[3000], op.name=B): Tensor(shape=[3000], op.name=compute.B.grad),\n",
       " Tensor(shape=[32, 3000], op.name=compute): Tensor(shape=[32, 3000], op.name=compute_red.compute.grad),\n",
       " Tensor(shape=[32, 3000], op.name=compute): Tensor(shape=[32, 3000], op.name=compute.compute.grad),\n",
       " Tensor(shape=[], op.name=compute_red): Tensor(shape=[], op.name=identity)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.adjoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each adjoint may be a sum of several components when there are several dependency paths from the output to the tensor. To access each component there is a dict called `adjoint_summands`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The adjoint of X is a sum:\n",
      "(compute.X.grad(ax0) + compute.X.grad(ax0))\n",
      "\n",
      "The component that came from A:\n",
      "reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0.000000f]), source=[(tensor.compute.grad(k0)*(select((ax0 == k0), 1.000000f, 0.000000f) + select(((ax0 + k0) == 9), 1.000000f, 0.000000f)))], axis=[iter_var(k0, Range(min=0, extent=10))], where=(uint1)1, value_index=0)\n",
      "\n",
      "The component that came from B:\n",
      "reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0.000000f]), source=[(tensor.compute.grad(k0)*(select((ax0 == k0), X((9 - k0)), 0.000000f) + select(((k0 + ax0) == 9), X(k0), 0.000000f)))], axis=[iter_var(k0, Range(min=0, extent=10))], where=(uint1)1, value_index=0)\n"
     ]
    }
   ],
   "source": [
    "X = tvm.placeholder((10,), name='X')\n",
    "A = tvm.compute((10,), lambda i: X[i] + X[9 - i])\n",
    "B = tvm.compute((10,), lambda i: X[i] * X[9 - i])\n",
    "Y = topi.tensordot(A, B, 1)\n",
    "\n",
    "# If we don't specify the inputs, it will differentiate\n",
    "# with respect to all tensors\n",
    "res = tvm.differentiate(Y)\n",
    "\n",
    "print(\"The adjoint of X is a sum:\")\n",
    "print(res.adjoints[X].op.body[0])\n",
    "\n",
    "print(\"\\nThe component that came from A:\")\n",
    "print(res.adjoint_summands[X][A].op.body[0])\n",
    "\n",
    "print(\"\\nThe component that came from B:\")\n",
    "print(res.adjoint_summands[X][B].op.body[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`adjoints` and `adjoint_summands` may probably be useful when manually scheduling the resulting computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works internally\n",
    "Internally `tvm.differentiate` recursively builds adjoints using the `tvm.autodiff.DiffBuildingBlock` function. This function takes three parameters: `output`, `input` and `head`. `output` is some tensor for which the adjoint has already been computed, and `head` is its adjoint. `input` is a tensor for which we want to compute the adjoint, and which is used from within the compute body of `output`. The function returns something close to `tensordot(head, Jacobian())` (but heavily optimized) where `Jacobian(Y, W)` simply differentiates `Y` wrt `W` assuming that `Y` depens on `W` only directly. So let's look at the `Jacobian` function. It has additional parameter which indicates whether to perform optimizations, so let's look at an unoptimized result of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The origiginal tensor Y:\n",
      "tensor compute{0xb92eb0}[0] : float32 [32, 3000]\n",
      "axes (i : [0, 31], j : [0, 2999])\n",
      "Reduction\n",
      "    identity [0.000000f]\n",
      "    lhs [x]  rhs [y]\n",
      "    combiner [(x + y)]\n",
      "    axes (k : [0, 9999])\n",
      "    condition (uint1)1\n",
      "    source[0] = (X(i, k)*W(j, k))\n",
      "\n",
      "tensor X{0x1dde3a0}[0] : float32 [32, 10000]\n",
      "    placeholder(X, 0x1dde3a0)\n",
      "\n",
      "tensor W{0x1ca8660}[0] : float32 [3000, 10000]\n",
      "    placeholder(W, 0x1ca8660)\n",
      "\n",
      "\n",
      "\n",
      "Jacobian(Y, W):\n",
      "tensor compute.jacobian{0x165b360}[0] : float32 [32, 3000, 3000, 10000]\n",
      "axes (i : [0, 31], j : [0, 2999], jac_i0 : [0, 2999], jac_i1 : [0, 9999])\n",
      "Reduction\n",
      "    identity [0.000000f]\n",
      "    lhs [x.der]  rhs [y.der]\n",
      "    combiner [(x.der + y.der)]\n",
      "    axes (k : [0, 9999])\n",
      "    condition (uint1)1\n",
      "    source[0] = (X(i, k)*float32(((jac_i0 == j) && (jac_i1 == k))))\n",
      "\n",
      "tensor X{0x1dde3a0}[0] : float32 [32, 10000]\n",
      "    placeholder(X, 0x1dde3a0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = tvm.placeholder((32, 10000), name='X')\n",
    "W = tvm.placeholder((3000, 10000), name='W')\n",
    "\n",
    "Y = topi.nn.dense(X, W)\n",
    "dYdW = tvm.autodiff.Jacobian(Y, W, False)\n",
    "\n",
    "# This function prints out a tensor with all its dependencies in a slightly more readable\n",
    "# format, in particular, it prints every attribute of a reduction on a new line\n",
    "print(\"The origiginal tensor Y:\")\n",
    "print(tvm.PrintTensorRecursively(Y))\n",
    "print(\"\\nJacobian(Y, W):\")\n",
    "print(tvm.PrintTensorRecursively(dYdW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `W(j, k)` in the original tensor Y became `float32(((jac_i0 == j) && (jac_i1 == k)))` in the Jacobian, which is the derivative of `W(j, k)` wrt `W(jac_i0, jac_i1)` (it's equal to 1 if the corresponding indices coincide, otherwise it's zero). Of course, computing this Jacobian is very inefficient, because it consists of summing over mostly zero values, so it should be optimized by propagating the information that `jac_i1 == k` and completely removing the summation. It may be done with the function `OptimizeAndLiftNonzeronessConditions` (which is called by the `Jacobian` function by default). Let's call it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor compute.jacobian{0x1996200}[0] : float32 [32, 3000, 3000, 10000]\n",
      "axes (i : [0, 31], j : [0, 2999], jac_i0 : [0, 2999], jac_i1 : [0, 9999])\n",
      "    select((j == jac_i0), tensor(jac_i1, i), 0.000000f)\n",
      "\n",
      "tensor tensor{0x201c2c0}[0] : float32 [10000, 32]\n",
      "axes (jac_i1 : [0, 9999], i : [0, 31])\n",
      "    X(i, jac_i1)\n",
      "\n",
      "tensor X{0x1dde3a0}[0] : float32 [32, 10000]\n",
      "    placeholder(X, 0x1dde3a0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dYdW_optimized = tvm.ir_pass.OptimizeAndLiftNonzeronessConditions(dYdW)\n",
    "print(tvm.PrintTensorRecursively(dYdW_optimized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction was eliminated completely, and replaced with a conditional expression returning `X(i, jac_i1)` if `j == jac_i0` and 0 otherwise.\n",
    "\n",
    "The condition `j == jac_i0` may be used to eliminate another reduction. Recall that the Jacobian is used in a formula looking similar to `tensordot(H, Jacobian(Y, W))`, so the reduction to be eliminated is a summation used in matrix multiplication. To perform this transformation, `JacobianRecursive` inlines the Jacobian and calls `OptimizeAndLiftNonzeronessConditions` once more. Let's do this manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor tensor{0x1fbbc90}[0] : float32 [3000, 10000]\n",
      "axes (ax0 : [0, 2999], ax1 : [0, 9999])\n",
      "    extracted(ax0, ax1)\n",
      "\n",
      "tensor extracted{0x1fd3e60}[0] : float32 [3000, 10000]\n",
      "axes (ax0 : [0, 2999], ax1 : [0, 9999])\n",
      "Reduction\n",
      "    identity [0.000000f]\n",
      "    lhs [x]  rhs [y]\n",
      "    combiner [(x + y)]\n",
      "    axes (k0 : [0, 31])\n",
      "    condition (uint1)1\n",
      "    source[0] = (H(k0, ax0)*tensor(ax1, k0))\n",
      "\n",
      "tensor H{0x1d758f0}[0] : float32 [32, 3000]\n",
      "    placeholder(H, 0x1d758f0)\n",
      "\n",
      "tensor tensor{0x201c2c0}[0] : float32 [10000, 32]\n",
      "axes (jac_i1 : [0, 9999], i : [0, 31])\n",
      "    X(i, jac_i1)\n",
      "\n",
      "tensor X{0x1dde3a0}[0] : float32 [32, 10000]\n",
      "    placeholder(X, 0x1dde3a0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generalized matmul works with tensors of arbitrary dimensions and takes\n",
    "# an additional parameter: the number of dimensions to contract. It is\n",
    "# semantically equivalent to reshaping into two matrices, \n",
    "# performing matrix multiplication, and then reshaping back\n",
    "dLdW = topi.tensordot(H, dYdW_optimized, 2)\n",
    "\n",
    "# We have to inline dYdW_optimized because OptimizeAndLiftNonzeronessConditions works\n",
    "# only with a single tensor\n",
    "dLdW_inlined = tvm.ir_pass.InlineNonReductions(dLdW, [dYdW_optimized])\n",
    "\n",
    "# Perform the main optimization\n",
    "dLdW_optimized = tvm.ir_pass.OptimizeAndLiftNonzeronessConditions(dLdW_inlined)\n",
    "print(tvm.PrintTensorRecursively(dLdW_optimized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that now there is only one reduction axis left, an there no comparison `j == jac_i0` anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing some gradients manually\n",
    "Sometimes automatic differentiation does poor job and it may be desired to provide a custom differentiation function for certain tensors. By default `tvm.differentiate` uses the function `tvm.autodiff.DiffBuildingBlock`, but it is possible to provide a custom function using the `fdiff` parameter. It is also possible to change the differentiation function only for some specific cases using the `manual` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Autodiff couldn't simplify some expressions:\n",
      "tensor compute.compute.grad{0x1496e40}[0] : float32 [32, 10, 26, 26]\n",
      "axes (ax0 : [0, 31], ax1 : [0, 9], ax2 : [0, 25], ax3 : [0, 25])\n",
      "    select(((((((((((((ax1*676) - (ax2*26)) - (select((0 < (((ax1*676) - ax3) - (ax2*26))), ax1, (ax1 + (((0 - ax3) - (ax2*26))/676)))*676)) <= ax3) && ((((ax3 + (ax2*26)) + (select((0 < (((ax1*676) - ax3) - (ax2*26))), ax1, (ax1 + (((0 - ax3) - (ax2*26))/676)))*676)) - (ax1*676)) <= 675)) && (((ax1*676) - ax3) <= ((ax2*26) + (select((0 < (((ax1*676) - ax3) - (ax2*26))), ax1, (ax1 + (((0 - ax3) - (ax2*26))/676)))*676)))) && ((((ax3 + (ax2*26)) + (select((0 < (((ax1*676) - ax3) - (ax2*26))), ax1, (ax1 + (((0 - ax3) - (ax2*26))/676)))*676)) - 675) <= (ax1*676))) && ((((ax1*676) - (ax2*26)) - (select((0 < (((ax1*676) - ax3) - (ax2*26))), ax1, (ax1 + (((0 - ax3) - (ax2*26))/676)))*676)) <= ax3)) && ((((ax3 + (ax2*26)) + (select((0 < (((ax1*676) - ax3) - (ax2*26))), ax1, (ax1 + (((0 - ax3) - (ax2*26))/676)))*676)) - 675) <= (ax1*676))) && ((((ax1*676) - ax3) - (ax2*26)) <= (select((0 < (((ax1*676) - ax3) - (ax2*26))), ax1, (ax1 + (((0 - ax3) - (ax2*26))/676)))*676))) && (0 <= select((0 < (((ax1*676) - ax3) - (ax2*26))), ax1, (ax1 + (((0 - ax3) - (ax2*26))/676))))) && ((((ax3 + (ax2*26)) + (select((0 < (((ax1*676) - ax3) - (ax2*26))), ax1, (ax1 + (((0 - ax3) - (ax2*26))/676)))*676)) - 675) <= (ax1*676))), compute_red.compute.grad(ax0, ((ax3 + (ax2*26)) + (select((0 < (((ax1*676) - ax3) - (ax2*26))), ax1, (ax1 + (((0 - ax3) - (ax2*26))/676)))*676))), 0.000000f)\n",
      "\n",
      "tensor compute_red.compute.grad{0x1f32340}[0] : float32 [32, 6760]\n",
      "axes (ax0 : [0, 31], ax1 : [0, 6759])\n",
      "    float32((uint1)1)\n",
      "\n",
      "\n",
      "======= We can simplify them manually:\n",
      "tensor compute{0x1e67790}[0] : float32 [32, 10, 26, 26]\n",
      "axes (ax0 : [0, 31], ax1 : [0, 9], ax2 : [0, 25], ax3 : [0, 25])\n",
      "    compute_red.compute.grad(ax0, ((ax3 + (ax2*26)) + (ax1*676)))\n",
      "\n",
      "tensor compute_red.compute.grad{0x1dd3730}[0] : float32 [32, 6760]\n",
      "axes (ax0 : [0, 31], ax1 : [0, 6759])\n",
      "    float32((uint1)1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = tvm.placeholder((32, 3, 28, 28), name='x')\n",
    "w1 = tvm.placeholder((10, 3, 3, 3), name='w1')\n",
    "t1 = topi.nn.conv2d(x, w1, 1, 0, 1)\n",
    "t2 = topi.nn.flatten(t1)\n",
    "t3 = topi.sum(t2)\n",
    "t3\n",
    "\n",
    "# Currently flatten is differentiated quite well, but there is an\n",
    "# annoying big condition that cannot be simplified\n",
    "res = tvm.differentiate(t3)\n",
    "print(\"======= Autodiff couldn't simplify some expressions:\")\n",
    "print(tvm.PrintTensorRecursively(res.adjoints[t1]))\n",
    "\n",
    "def mydiff(out, inp, head):\n",
    "    return tvm.compute(inp.shape, lambda ax0, ax1, ax2, ax3: head[ax0, ax3 + ax2*26 + ax1*676])\n",
    "\n",
    "# Here we provide the better version manually by specifying that\n",
    "# for the pair of tensors (t2, t1), t2 being the output, and t1 the input,\n",
    "# the function `mydiff` should be used instead of DiffBuildingBlock\n",
    "print(\"======= We can simplify them manually:\")\n",
    "res = tvm.differentiate(t3, [x, w1], manual={(t2, t1): mydiff})\n",
    "print(tvm.PrintTensorRecursively(res.adjoints[t1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supported operations\n",
    "Here is a list of operations which seem to be differentiated quite well by our autodiff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tvm.placeholder((32, 1000), name='X')\n",
    "W = tvm.placeholder((1000, 1000), name='W')\n",
    "B = tvm.placeholder((1000,), name='B')\n",
    "\n",
    "Y = topi.nn.dense(X, W, B)\n",
    "\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "grads = tvm.differentiate(Y, [X, W, B], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  26ms median, avg=27, std=5 (36 iters)\n",
      "backward 54ms median, avg=57, std=11 (18 iters)\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X, B, W]))\n",
    "print(\"backward\", measure_performance(list(grads), [X, B, W, H]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2D\n",
    "with `dilation=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tvm.placeholder((32, 17, 28, 28), name='X')\n",
    "W = tvm.placeholder((19, 17, 3, 3), name='W')\n",
    "Y = topi.nn.conv2d(X, W, [1, 1], [0, 0], 1)\n",
    "\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "grads = tvm.differentiate(Y, [X, W], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  58ms median, avg=59, std=9 (17 iters)\n",
      "backward 102ms median, avg=109, std=19 (10 iters)\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X, W]))\n",
    "print(\"backward\", measure_performance(list(grads), [X, W, H]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  78ms median, avg=80, std=5 (13 iters)\n",
      "backward 172ms median, avg=191, std=42 (6 iters)\n"
     ]
    }
   ],
   "source": [
    "X = tvm.placeholder((32, 17, 58, 58), name='X')\n",
    "W = tvm.placeholder((19, 17, 5, 5), name='W')\n",
    "Y = topi.nn.conv2d(X, W, [3, 3], [1, 1], 1)\n",
    "\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "grads = tvm.differentiate(Y, [X, W], H)\n",
    "\n",
    "print(\"forward \", measure_performance([Y], [X, W]))\n",
    "print(\"backward\", measure_performance(list(grads), [X, W, H]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Somewhat supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average pooling\n",
    "The performance is suspicious but the generated code looks ok except for large if expressions which cannot be eliminated by subsequent passes (the problem in not nearly as horrible as with max pooling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tvm.placeholder((32, 17, 280, 280), name='X')\n",
    "Y = topi.nn.pool(X, [2, 2], [2, 2], [0, 0, 0, 0], 'avg')\n",
    "\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "grads = tvm.differentiate(Y, [X], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  13ms median, avg=14, std=2 (72 iters)\n",
      "backward 94ms median, avg=98, std=14 (11 iters)\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X]))\n",
    "print(\"backward\", measure_performance(list(grads), [X, H]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor tensor.X.grad{0x22c5180}[0] : float32 [32, 17, 280, 280]\n",
      "axes (ax0 : [0, 31], ax1 : [0, 16], ax2 : [0, 279], ax3 : [0, 279])\n",
      "    select((((((((ax2 - 1) <= (select((1 < ax2), (ax2/2), ((ax2 - 1)/2))*2)) && (0 <= select((1 < ax2), (ax2/2), ((ax2 - 1)/2)))) && ((select((1 < ax2), (ax2/2), ((ax2 - 1)/2))*2) <= ax2)) && ((ax3 - 1) <= (select((1 < ax3), (ax3/2), ((ax3 - 1)/2))*2))) && (0 <= select((1 < ax3), (ax3/2), ((ax3 - 1)/2)))) && ((select((1 < ax3), (ax3/2), ((ax3 - 1)/2))*2) <= ax3)), (H(ax0, ax1, select((1 < ax2), (ax2/2), ((ax2 - 1)/2)), select((1 < ax3), (ax3/2), ((ax3 - 1)/2)))*0.250000f), 0.000000f)\n",
      "\n",
      "tensor H{0x23c26e0}[0] : float32 [32, 17, 140, 140]\n",
      "    placeholder(H, 0x23c26e0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.PrintTensorRecursively(grads[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax (homebrewn)\n",
    "Softmax from topi causes performance problems (see below), but we can write our own softmax which works better but still not perfectly (seems like some performance problems when used after other layers like dense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tvm.placeholder((60, 100), name=\"X\")\n",
    "W = tvm.placeholder((1000, 1000), name='W')\n",
    "\n",
    "exps = topi.exp(topi.nn.dense(X, W))\n",
    "sumexps = topi.sum(exps, axis=-1, keepdims=True)\n",
    "Y = exps/sumexps\n",
    "\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "grads = tvm.differentiate(Y, [X, W], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  6ms median, avg=6, std=2 (154 iters)\n",
      "backward 18ms median, avg=19, std=4 (53 iters)\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X, W]))\n",
    "print(\"backward\", measure_performance(list(grads), [X, W, H]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten\n",
    "Flatten uses the division and modulo operations. Recently we've implemented a transformation to deal with them, so this operation is pretty much supported, although the resulting code is not perfect (it contains a huge select which should be simplified out in theory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tvm.placeholder((32, 100, 20, 25), name='X')\n",
    "Y = topi.nn.flatten(X)\n",
    "\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "grads = tvm.differentiate(Y, [X], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  5ms median, avg=6, std=2 (165 iters)\n",
      "backward 7ms median, avg=8, std=3 (123 iters)\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X]))\n",
    "print(\"backward\", measure_performance(list(grads), [X, H]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor compute.X.grad{0x2071170}[0] : float32 [32, 100, 20, 25]\n",
      "axes (ax0 : [0, 31], ax1 : [0, 99], ax2 : [0, 19], ax3 : [0, 24])\n",
      "    select(((((((((((((ax1*500) - (ax2*25)) - (select((0 < (((ax1*500) - ax3) - (ax2*25))), ax1, (ax1 + (((0 - ax3) - (ax2*25))/500)))*500)) <= ax3) && ((((ax3 + (ax2*25)) + (select((0 < (((ax1*500) - ax3) - (ax2*25))), ax1, (ax1 + (((0 - ax3) - (ax2*25))/500)))*500)) - (ax1*500)) <= 499)) && (((ax1*500) - ax3) <= ((ax2*25) + (select((0 < (((ax1*500) - ax3) - (ax2*25))), ax1, (ax1 + (((0 - ax3) - (ax2*25))/500)))*500)))) && ((((ax3 + (ax2*25)) + (select((0 < (((ax1*500) - ax3) - (ax2*25))), ax1, (ax1 + (((0 - ax3) - (ax2*25))/500)))*500)) - 499) <= (ax1*500))) && ((((ax1*500) - (ax2*25)) - (select((0 < (((ax1*500) - ax3) - (ax2*25))), ax1, (ax1 + (((0 - ax3) - (ax2*25))/500)))*500)) <= ax3)) && ((((ax3 + (ax2*25)) + (select((0 < (((ax1*500) - ax3) - (ax2*25))), ax1, (ax1 + (((0 - ax3) - (ax2*25))/500)))*500)) - 499) <= (ax1*500))) && ((((ax1*500) - ax3) - (ax2*25)) <= (select((0 < (((ax1*500) - ax3) - (ax2*25))), ax1, (ax1 + (((0 - ax3) - (ax2*25))/500)))*500))) && (0 <= select((0 < (((ax1*500) - ax3) - (ax2*25))), ax1, (ax1 + (((0 - ax3) - (ax2*25))/500))))) && ((((ax3 + (ax2*25)) + (select((0 < (((ax1*500) - ax3) - (ax2*25))), ax1, (ax1 + (((0 - ax3) - (ax2*25))/500)))*500)) - 499) <= (ax1*500))), H(ax0, ((ax3 + (ax2*25)) + (select((0 < (((ax1*500) - ax3) - (ax2*25))), ax1, (ax1 + (((0 - ax3) - (ax2*25))/500)))*500))), 0.000000f)\n",
      "\n",
      "tensor H{0x221e0a0}[0] : float32 [32, 50000]\n",
      "    placeholder(H, 0x221e0a0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.PrintTensorRecursively(grads[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the compiler has to figure out that `k1.shifted` is directly expressible using `ax1, ax2, ax3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max pooling\n",
    "Reducing with other combiners, like max, is a bit trickier than summation. We used to have a problem with memory allocation: differentiating max pooling resulted in tesors larger than 2^31, but the problem is fixed now. Still, it's not very performant, the reason is yet unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tvm.placeholder((32, 64, 100, 100), name='X')\n",
    "Y = topi.nn.pool(X, [2, 2], [2, 2], [0, 0, 0, 0], 'max')\n",
    "\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "grads = tvm.differentiate(Y, [X], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  7ms median, avg=7, std=1 (135 iters)\n",
      "backward 765ms median, avg=765, std=7 (2 iters)\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X]))\n",
    "print(\"backward\", measure_performance(list(grads), [X, H]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "Softmax uses max behind the scenes, causing some performance problems. We havent't yet investingated into it though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tvm.placeholder((60, 100), name=\"X\")\n",
    "W = tvm.placeholder((1000, 1000), name='W')\n",
    "\n",
    "Y = topi.nn.softmax(topi.nn.dense(X, W))\n",
    "\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "grads = tvm.differentiate(Y, [X, W], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  7ms median, avg=8, std=2 (122 iters)\n",
      "backward 183ms median, avg=187, std=28 (6 iters)\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X, W]))\n",
    "print(\"backward\", measure_performance(list(grads), [X, W, H]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupported\n",
    "Currently **dilated convolutions are not supported** (wrong gradients, probably a simplification bug or something like this)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
